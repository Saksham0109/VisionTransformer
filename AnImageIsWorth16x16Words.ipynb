{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Image is worth 16 x 16 Words\n",
    "This research paper talks about the application of Tranformers architecture(which is already popular in NLP) in CV tasks.\n",
    "\n",
    "The paper shows that CNNs is not necessary and pure transformer architecture can perform well.\n",
    "\n",
    "The model architecture developed ViT(Vision Transformer) attains State of the Art results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper talks about how when the architecture is applied on datasets like ImageNet then the accuracy achieved is lesser than ResNet of similiar size.This is because Transformers lack some of the inherent inductive bias of CNN like translation equivariance.\n",
    "\n",
    "However when pre-trained on huge dataset like ImageNet 21k or JFT-300M then the Transformers achieve state of the art results.\n",
    "\n",
    "The writers have tried to keep the architecture as close to the one mentioned in the famous \"Attention is all you need\" paper which was the first to describe the tranformer architecture.\n",
    "\n",
    "We will implement this model in Pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange,reduce,repeat\n",
    "#Einops have been used for Rearrangement of size of tensors as well as for reduction and repeating\n",
    "#I found Einops to be a very intuitive way to carry out these operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to input an Image as a sequence\n",
    "The thing about Tranformers which i will be describing in a bit is that the input is a sequence (words in a sentence in case of NLP) for which we divide the image into patches for which the patch has a size 2x2.So for a image with height H and width W we have N=H*W/P^2(N:Number of Patches).Each patch is of Size [P,P,C] where P is patch size and C is number of channel.\n",
    "\n",
    "These patches are then flattened into 1D Tensor and then Position embedding is added to them(These patches as it is dont have knowledge of the position of the patch in the picture therefore we add Position embedding to them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,height,width,in_channel,embed_size):\n",
    "        super(Embedding,self).__init__()\n",
    "        self.height=height\n",
    "        self.width=width\n",
    "        self.in_channel=in_channel\n",
    "        self.embed_size=embed_size\n",
    "        self.projection =nn.Conv2d(in_channel, embed_size, kernel_size=2, stride=2)\n",
    "        #authors are using a Conv2d layer instead of a Linear one for performance gain. \n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, (height*width)//4 + 1, embed_size))\n",
    "        #spatial information\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_size))\n",
    "        #cls_token:number placed in front of each sequence (of projected patches)\n",
    "\n",
    "    def forward(self,x):\n",
    "        b,c,h,w=x.shape\n",
    "        x=self.projection(x)\n",
    "        x=rearrange(x,'b e (h) (w) -> b (h w) e')\n",
    "        b,n,_=x.shape\n",
    "        cls_token=repeat(self.cls_token,'1 1 d -> b 1 d',b=b)\n",
    "        x=torch.cat((cls_token,x),dim=1)\n",
    "        x+=self.pos_embedding\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "As described in \"Attention is all you need\" we define Queries,Key and Value which we get by passing the input through 3 linear layer.Attention is defined= Softmax((Queries * Key.T)/sqrt(embedding size))*Value.\n",
    "\n",
    "This layer allows each pixel to define how much attention should be given to each other pixel with respect to it.\n",
    "\n",
    "## MultiHead Attention\n",
    "What was realized was that results were better when you averaged over different attentions.In order to not increase computation time what was thought was that the dimension(in channel) of the input should be divided among the different heads and this is called MultiHead Attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,emb_size,num_heads=8,dropout=0):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        queries=rearrange(self.queries(x),\"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        keys=rearrange(self.keys(x),\"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        values=rearrange(self.values(x),\"b n (h d) -> b h n d\", h=self.num_heads)\n",
    "        a=torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "\n",
    "        att = F.softmax(a, dim=-1) /(self.emb_size**0.5)\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out+=x #Shortcut connection used\n",
    "        out = self.projection(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer block also consists of a feed forward block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, emb_size, expansion = 4, drop_p= 0):\n",
    "        super(FeedForwardBlock,self).__init__()\n",
    "        self.block=nn.Sequential(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size))\n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.block(x)\n",
    "        out+=x #Shortcut connection used\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "The transformer Block consists of The self attention layer followed by the feed forward layer with layer normalization happening before every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self,emb_size=768,drop_p=0,forward_expansion=4,forward_drop_p=0):\n",
    "        super(TransformerEncoderBlock,self).__init__()\n",
    "        self.block=nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size),\n",
    "                nn.Dropout(drop_p),\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        out=self.block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the transformer blocks,we have a classification block in the end which consists of a linear layer which give us an output equal to number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self, emb_size= 768, n_classes= 1000):\n",
    "        super(Classification,self).__init__()\n",
    "        self.block=nn.Sequential(\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))\n",
    "    def forward(self,x):\n",
    "        out=reduce(x,'b n e -> b e', reduction='mean')\n",
    "        out=self.block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "Finally we build our model by combining all these layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self,     \n",
    "                in_channels= 3,\n",
    "                patch_size= 16,\n",
    "                emb_size= 768,\n",
    "                img_size= 224,\n",
    "                depth= 12,\n",
    "                n_classes= 1000):\n",
    "        super(ViT,self).__init__()\n",
    "        self.embedding=Embedding(224, 224, in_channels, emb_size)\n",
    "        self.block=self.make_layer(depth)\n",
    "        self.classification=Classification(emb_size, n_classes)\n",
    "\n",
    "    def make_layer(self,depth):\n",
    "        layers=[]\n",
    "        for i in range(depth):\n",
    "            layers.append(TransformerEncoderBlock())\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.embedding(x)\n",
    "        out=self.block(out)\n",
    "        out=self.classification(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2070c5131435e64915ea90deb81f12392f7964ea8fe444034525e252d38eab81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
